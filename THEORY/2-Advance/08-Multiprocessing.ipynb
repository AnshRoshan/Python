{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚙️ Mastering Multiprocessing in Python: True Parallelism for CPU-Bound Tasks\n",
    "\n",
    "**Welcome!** This notebook explores Python's `multiprocessing` module, the standard library's solution for achieving true parallelism by creating and managing multiple independent processes. Unlike threading (in CPython), multiprocessing bypasses the Global Interpreter Lock (GIL), making it ideal for CPU-bound tasks that can benefit from multiple cores.\n",
    "\n",
    "**Target Audience:** Python developers needing to accelerate CPU-intensive computations, process large datasets in parallel, or fully utilize multi-core processors.\n",
    "\n",
    "**Learning Objectives:**\n",
    "*   Understand the process model and its difference from threading (separate memory).\n",
    "*   Create, start, and manage `Process` objects.\n",
    "*   Learn various Inter-Process Communication (IPC) mechanisms: `Queue`, `Pipe`, `Value`, `Array`, `Manager`.\n",
    "*   Use synchronization primitives (`Lock`, `Event`, `Semaphore`, etc.) between processes.\n",
    "*   Effectively manage pools of worker processes using `multiprocessing.Pool`.\n",
    "*   Utilize the high-level `concurrent.futures.ProcessPoolExecutor`.\n",
    "*   Understand different process start methods (`fork`, `spawn`, `forkserver`).\n",
    "*   Identify best practices, pitfalls (IPC overhead, serialization), and performance considerations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction: Why Multiprocessing?\n",
    "\n",
    "As established previously, CPython's Global Interpreter Lock (GIL) prevents threads within the same process from executing Python bytecode simultaneously on multiple CPU cores. This makes standard threading unsuitable for speeding up CPU-bound tasks.\n",
    "\n",
    "**Multiprocessing provides a solution:** It creates *separate processes*, each with its own Python interpreter and memory space. Since each process has its own GIL, multiple processes can run Python code **in parallel** on different CPU cores.\n",
    "\n",
    "**Key Use Case: CPU-Bound Tasks**\n",
    "The primary benefit of multiprocessing is accelerating tasks that are limited by CPU speed, such as:\n",
    "*   Complex mathematical calculations\n",
    "*   Image/video processing\n",
    "*   Data analysis and manipulation on large datasets\n",
    "*   Scientific simulations\n",
    "*   Machine learning model training (often handled by libraries using multiprocessing internally)\n",
    "\n",
    "**Analogy: Hiring More Independent Workers**\n",
    "\n",
    "Instead of having multiple employees (threads) in one office sharing the same (sometimes bottlenecked) resources and rules (GIL), multiprocessing is like setting up completely separate, independent workshops (processes). Each workshop has its own tools and interpreter (memory and GIL) and can work fully in parallel on its assigned task. However, communication and sharing materials between workshops (IPC) requires more effort (sending messages, using shared storage) than workers talking in the same room."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Creating and Managing Processes (`multiprocessing.Process`)\n",
    "\n",
    "The interface is very similar to `threading.Thread`.\n",
    "\n",
    "**Steps:**\n",
    "1.  Define a target function for the process.\n",
    "2.  Create a `Process` instance, specifying `target` and `args`/`kwargs`.\n",
    "3.  Call `start()` to spawn the new process.\n",
    "4.  Call `join()` to wait for the process to finish.\n",
    "\n",
    "**Important:** Due to how processes are often created (especially `spawn` on Windows/macOS), the main part of your script that creates processes **must** be protected by `if __name__ == '__main__':`. This prevents infinite process creation when the child process re-imports the main script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] (MainProcess - PID 10985) Main process starting.\n",
      "[INFO] (MainProcess - PID 10985) Creating 4 processes...\n",
      "[INFO] (MainProcess - PID 10985) Started process WorkerProc-1 (PID: 10997)\n",
      "[INFO] (WorkerProc-1 - PID 10997) Starting task 1...\n",
      "[INFO] (MainProcess - PID 10985) Started process WorkerProc-2 (PID: 11000)\n",
      "[INFO] (WorkerProc-2 - PID 11000) Starting task 2...\n",
      "[INFO] (MainProcess - PID 10985) Started process WorkerProc-3 (PID: 11003)\n",
      "[INFO] (WorkerProc-3 - PID 11003) Starting task 3...\n",
      "[INFO] (MainProcess - PID 10985) Started process WorkerProc-4 (PID: 11006)\n",
      "[INFO] (MainProcess - PID 10985) Waiting for processes to join...\n",
      "[INFO] (WorkerProc-4 - PID 11006) Starting task 4...\n",
      "[INFO] (WorkerProc-4 - PID 11006) Finished task 4. Result sum part: 0\n",
      "[INFO] (WorkerProc-3 - PID 11003) Finished task 3. Result sum part: 0\n",
      "[INFO] (WorkerProc-1 - PID 10997) Finished task 1. Result sum part: 0\n",
      "[INFO] (MainProcess - PID 10985) Process WorkerProc-1 finished with exit code 0\n",
      "[INFO] (WorkerProc-2 - PID 11000) Finished task 2. Result sum part: 0\n",
      "[INFO] (MainProcess - PID 10985) Process WorkerProc-2 finished with exit code 0\n",
      "[INFO] (MainProcess - PID 10985) Process WorkerProc-3 finished with exit code 0\n",
      "[INFO] (MainProcess - PID 10985) Process WorkerProc-4 finished with exit code 0\n",
      "[INFO] (MainProcess - PID 10985) All processes finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total execution time (multiprocessing): 2.56 seconds\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, current_process\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "\n",
    "# Basic logging config (might show duplicate messages from child processes\n",
    "# depending on start method and OS, more robust logging needed for prod)\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='[%(levelname)s] (%(processName)s - PID %(process)d) %(message)s',\n",
    "                    force=True)\n",
    "\n",
    "def cpu_bound_task(task_id: int, count_to: int):\n",
    "    \"\"\"Simulates a CPU-intensive task.\"\"\"\n",
    "    process_name = current_process().name\n",
    "    pid = os.getpid()\n",
    "    logging.info(f\"Starting task {task_id}...\")\n",
    "    result = 0\n",
    "    for i in range(count_to):\n",
    "        result += i * i # Some calculation\n",
    "    logging.info(f\"Finished task {task_id}. Result sum part: {result % 1000}\") # Print part of result\n",
    "\n",
    "# --- Main Guard --- \n",
    "if __name__ == \"__main__\":\n",
    "    logging.info(\"Main process starting.\")\n",
    "    \n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    processes: list[Process] = []\n",
    "    num_processes = min(os.cpu_count() or 1, 4) # Use up to 4 cores for demo\n",
    "    count_target = 10_000_000 # Number large enough to take some time\n",
    "    \n",
    "    logging.info(f\"Creating {num_processes} processes...\")\n",
    "    for i in range(num_processes):\n",
    "        # Create process\n",
    "        # daemon=True: process exits if parent exits (use with caution)\n",
    "        process = Process(target=cpu_bound_task, args=(i + 1, count_target), \n",
    "                          name=f\"WorkerProc-{i+1}\")\n",
    "        processes.append(process)\n",
    "        # Start the process\n",
    "        process.start()\n",
    "        logging.info(f\"Started process {process.name} (PID: {process.pid})\")\n",
    "        \n",
    "    # Wait for all processes to complete\n",
    "    logging.info(\"Waiting for processes to join...\")\n",
    "    for process in processes:\n",
    "        process.join()\n",
    "        logging.info(f\"Process {process.name} finished with exit code {process.exitcode}\")\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    logging.info(\"All processes finished.\")\n",
    "    print(f\"\\nTotal execution time (multiprocessing): {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    # Compare with sequential execution (conceptual)\n",
    "    # start_seq = time.perf_counter()\n",
    "    # cpu_bound_task(0, count_target * num_processes) # Roughly equivalent work\n",
    "    # end_seq = time.perf_counter()\n",
    "    # print(f\"Estimated sequential time: {end_seq - start_seq:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inter-Process Communication (IPC)\n",
    "\n",
    "Since processes have separate memory spaces, they cannot directly access each other's variables. The `multiprocessing` module provides several ways to communicate and share data:\n",
    "\n",
    "### 3.1 `multiprocessing.Queue`\n",
    "*   A **process-safe** queue, similar interface to `queue.Queue`.\n",
    "*   Data put onto the queue is pickled, transferred between processes (via OS mechanisms like pipes), and unpickled.\n",
    "*   Good for passing moderately sized, pickleable objects between processes (task distribution, result collection).\n",
    "*   **Note:** Can be slower than `threading`'s `queue.Queue` due to serialization overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multiprocessing Queue Demo ---\n",
      "(ProducerProc) Finished putting squares.\n",
      "Producer process finished.\n",
      "Approximate queue size: 10\n",
      "Retrieving results from queue:\n",
      "  Got: (Original=0, Square=0) from ProducerProc (PID 11024)\n",
      "  Got: (Original=1, Square=1) from ProducerProc (PID 11024)\n",
      "  Got: (Original=2, Square=4) from ProducerProc (PID 11024)\n",
      "  Got: (Original=3, Square=9) from ProducerProc (PID 11024)\n",
      "  Got: (Original=4, Square=16) from ProducerProc (PID 11024)\n",
      "  Got: (Original=5, Square=25) from ProducerProc (PID 11024)\n",
      "  Got: (Original=6, Square=36) from ProducerProc (PID 11024)\n",
      "  Got: (Original=7, Square=49) from ProducerProc (PID 11024)\n",
      "  Got: (Original=8, Square=64) from ProducerProc (PID 11024)\n",
      "  Got: (Original=9, Square=81) from ProducerProc (PID 11024)\n",
      "Queue demo finished.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue, current_process\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Note: Functions run by processes must be defined at the top level\n",
    "# or be importable. Lambdas often don't work well.\n",
    "def worker_puts_squares(numbers: list, q: Queue):\n",
    "    \"\"\"Calculates squares and puts them onto the queue.\"\"\"\n",
    "    pid = os.getpid()\n",
    "    proc_name = current_process().name\n",
    "    for n in numbers:\n",
    "        result = n * n\n",
    "        q.put((pid, proc_name, n, result))\n",
    "        time.sleep(0.01) # Simulate work\n",
    "    print(f\"({proc_name}) Finished putting squares.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Multiprocessing Queue Demo ---\")\n",
    "    \n",
    "    task_queue = Queue() # Process-safe queue\n",
    "    data = list(range(10))\n",
    "    \n",
    "    # Create and start a process to put data onto the queue\n",
    "    producer_process = Process(target=worker_puts_squares, args=(data, task_queue),\n",
    "                               name=\"ProducerProc\")\n",
    "    producer_process.start()\n",
    "    \n",
    "    # Main process gets data from the queue\n",
    "    # Wait for the producer to finish putting items before checking size\n",
    "    producer_process.join()\n",
    "    print(\"Producer process finished.\")\n",
    "    \n",
    "    print(f\"Approximate queue size: {task_queue.qsize()}\")\n",
    "    \n",
    "    print(\"Retrieving results from queue:\")\n",
    "    while not task_queue.empty():\n",
    "        try:\n",
    "            pid, proc_name, original, square = task_queue.get(timeout=1)\n",
    "            print(f\"  Got: (Original={original}, Square={square}) from {proc_name} (PID {pid})\")\n",
    "        except queue.Empty:\n",
    "            print(\"Queue became empty while getting.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting from queue: {e}\")\n",
    "            break\n",
    "            \n",
    "    print(\"Queue demo finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 `multiprocessing.Pipe`\n",
    "*   Returns a pair of `Connection` objects representing the two ends of a pipe.\n",
    "*   Each connection object has `send()` and `recv()` methods.\n",
    "*   Primarily for two-way communication between **two** specific processes.\n",
    "*   Data is pickled/unpickled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multiprocessing Pipe Demo ---\n",
      "Receiver: Waiting for messages...\n",
      "Sender: Sending messages...\n",
      "Receiver: Received --> Hello from sender!\n",
      "Receiver: Received --> [1, 2, {'data': 'payload'}]\n",
      "Sender: Done and closed.Receiver: Received None, stopping.\n",
      "\n",
      "Receiver: Closed.\n",
      "Pipe demo finished.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Pipe\n",
    "from multiprocessing.connection import Connection\n",
    "import time\n",
    "\n",
    "def sender(conn: Connection):\n",
    "    \"\"\"Sends messages through one end of the pipe.\"\"\"\n",
    "    print(\"Sender: Sending messages...\")\n",
    "    conn.send(\"Hello from sender!\")\n",
    "    time.sleep(0.5)\n",
    "    conn.send([1, 2, {'data': 'payload'}])\n",
    "    time.sleep(0.5)\n",
    "    conn.send(None) # Signal end\n",
    "    conn.close()\n",
    "    print(\"Sender: Done and closed.\")\n",
    "\n",
    "def receiver(conn: Connection):\n",
    "    \"\"\"Receives messages from the other end of the pipe.\"\"\"\n",
    "    print(\"Receiver: Waiting for messages...\")\n",
    "    while True:\n",
    "        try:\n",
    "            msg = conn.recv() # Blocks until message received\n",
    "            if msg is None: # Check for end signal\n",
    "                print(\"Receiver: Received None, stopping.\")\n",
    "                break\n",
    "            print(f\"Receiver: Received --> {msg}\")\n",
    "        except EOFError: # Raised if sender closes connection unexpectedly\n",
    "            print(\"Receiver: Connection closed by sender.\")\n",
    "            break\n",
    "    conn.close()\n",
    "    print(\"Receiver: Closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Multiprocessing Pipe Demo ---\")\n",
    "    \n",
    "    # Create the pipe (returns two connection objects)\n",
    "    parent_conn, child_conn = Pipe()\n",
    "    \n",
    "    # Create processes, passing one end of the pipe to each\n",
    "    p_sender = Process(target=sender, args=(child_conn,), name=\"SenderProc\")\n",
    "    p_receiver = Process(target=receiver, args=(parent_conn,), name=\"ReceiverProc\")\n",
    "    \n",
    "    p_receiver.start()\n",
    "    p_sender.start()\n",
    "    \n",
    "    p_sender.join()\n",
    "    p_receiver.join()\n",
    "    \n",
    "    # Close connections in the main process too (though child processes closed theirs)\n",
    "    parent_conn.close()\n",
    "    child_conn.close()\n",
    "    \n",
    "    print(\"Pipe demo finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Shared Memory (`Value`, `Array`)\n",
    "*   Allows processes to share data more directly using shared memory blocks managed by the OS.\n",
    "*   Requires specifying a data type (`ctypes`) for the shared object.\n",
    "*   Faster than Queue/Pipe for simple data types as it avoids pickling overhead.\n",
    "*   **Requires explicit locking (`multiprocessing.Lock`)** to prevent race conditions when multiple processes modify the shared memory concurrently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multiprocessing Shared Memory Demo ---\n",
      "Initial Value: 0\n",
      "Initial Array: [10.0, 20.0, 30.0]\n",
      "Final Value: 1000\n",
      "Final Array: [510.0, 520.0, 530.0]\n",
      "Shared memory demo finished.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Value, Array, Lock\n",
    "import time\n",
    "\n",
    "# Type codes: 'i' for signed int, 'd' for double-precision float, 'c' for char\n",
    "# See ctypes documentation for more types\n",
    "\n",
    "def modify_shared(shared_val: Value, shared_arr: Array, lock: Lock):\n",
    "    \"\"\"Modifies shared memory objects using a lock.\"\"\"\n",
    "    for _ in range(500): # Fewer iterations for demo speed\n",
    "        with lock: # Acquire lock\n",
    "            shared_val.value += 1\n",
    "            for i in range(len(shared_arr)):\n",
    "                shared_arr[i] += 0.5\n",
    "        # Lock automatically released\n",
    "        time.sleep(0.001) # Tiny sleep to increase chance of context switch\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Multiprocessing Shared Memory Demo ---\")\n",
    "    \n",
    "    # Create shared objects\n",
    "    shared_int = Value('i', 0) # Shared signed integer, initial value 0\n",
    "    shared_double_array = Array('d', [10.0, 20.0, 30.0]) # Shared double array\n",
    "    \n",
    "    # Create a lock for synchronization\n",
    "    lock = Lock()\n",
    "    \n",
    "    print(f\"Initial Value: {shared_int.value}\")\n",
    "    print(f\"Initial Array: {list(shared_double_array)}\")\n",
    "\n",
    "    p1 = Process(target=modify_shared, args=(shared_int, shared_double_array, lock))\n",
    "    p2 = Process(target=modify_shared, args=(shared_int, shared_double_array, lock))\n",
    "\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "\n",
    "    # Value should be 2 * 500 = 1000\n",
    "    # Each array element should be initial + 2 * 500 * 0.5 = initial + 500\n",
    "    print(f\"Final Value: {shared_int.value}\") \n",
    "    print(f\"Final Array: {list(shared_double_array)}\")\n",
    "    \n",
    "    print(\"Shared memory demo finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Server Process (`multiprocessing.Manager`)\n",
    "*   Starts a separate manager process.\n",
    "*   This manager process can host shared Python objects like lists, dicts, queues, etc. (`manager.list()`, `manager.dict()`).\n",
    "*   Other processes communicate with the manager process via proxies to access and modify these shared objects.\n",
    "*   More flexible than `Value`/`Array` (supports complex Python objects), but slower due to communication overhead with the manager process.\n",
    "*   Handles synchronization for standard methods on managed objects (e.g., `managed_list.append()` is process-safe)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multiprocessing Manager Demo ---\n",
      "Manager process started.\n",
      "Worker 1 (PID 11073) starting.\n",
      "Worker 2 (PID 11076) starting.\n",
      "Worker 1: Current dict view = {1: 11073}\n",
      "Worker 2: Current dict view = {1: 11073, 2: 11076}\n",
      "Worker 3 (PID 11083) starting.\n",
      "Worker 3: Current dict view = {1: 11073, 2: 11076, 3: 11083}\n",
      "Worker 1 finished.\n",
      "Worker 2 finished.\n",
      "Worker 3 finished.\n",
      "\n",
      "--- Final Managed Objects State ---\n",
      "Final Dict: {1: 11073, 2: 11076, 3: 11083}\n",
      "Final List: ['Data from 1', 'Data from 2', 'Data from 3']\n",
      "Manager demo finished.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Manager\n",
    "import time\n",
    "import os\n",
    "\n",
    "def worker_modifies_managed(managed_dict, managed_list, worker_id):\n",
    "    \"\"\"Modifies objects hosted by the Manager process.\"\"\"\n",
    "    pid = os.getpid()\n",
    "    print(f\"Worker {worker_id} (PID {pid}) starting.\")\n",
    "    \n",
    "    # Modifications via proxy objects are sent to the manager process\n",
    "    managed_dict[worker_id] = pid\n",
    "    managed_list.append(f\"Data from {worker_id}\")\n",
    "    \n",
    "    # Reading is also via proxy\n",
    "    print(f\"Worker {worker_id}: Current dict view = {dict(managed_dict)}\")\n",
    "    time.sleep(0.5)\n",
    "    print(f\"Worker {worker_id} finished.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Multiprocessing Manager Demo ---\")\n",
    "    \n",
    "    # Create a Manager object (starts a server process)\n",
    "    with Manager() as manager:\n",
    "        print(\"Manager process started.\")\n",
    "        \n",
    "        # Create managed shared objects\n",
    "        shared_dict = manager.dict() \n",
    "        shared_list = manager.list()\n",
    "        \n",
    "        processes = []\n",
    "        for i in range(3):\n",
    "            p = Process(target=worker_modifies_managed, \n",
    "                        args=(shared_dict, shared_list, i+1))\n",
    "            processes.append(p)\n",
    "            p.start()\n",
    "            \n",
    "        # Wait for worker processes to finish\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "            \n",
    "        # Access the final state of the managed objects (from main process)\n",
    "        print(\"\\n--- Final Managed Objects State ---\")\n",
    "        # Convert proxies to regular types for printing if needed\n",
    "        print(f\"Final Dict: {dict(shared_dict)}\")\n",
    "        print(f\"Final List: {list(shared_list)}\")\n",
    "        \n",
    "    # Manager process is automatically shut down when exiting the 'with' block\n",
    "    print(\"Manager demo finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Synchronization Between Processes\n",
    "\n",
    "Similar to `threading`, the `multiprocessing` module provides synchronization primitives like `Lock`, `RLock`, `Semaphore`, `Event`, `Condition`. Their API is nearly identical, but they operate across process boundaries.\n",
    "\n",
    "As shown in the `Value`/`Array` example (Section 3.3), these are crucial when using shared memory (`Value`, `Array`) to prevent race conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Pools (`multiprocessing.Pool`)\n",
    "\n",
    "Manages a pool of worker processes. Useful for distributing tasks across available CPU cores without manually managing individual `Process` objects.\n",
    "\n",
    "**Key Methods:**\n",
    "*   `pool.map(func, iterable, chunksize=None)`: Applies `func` to each item in `iterable`. Chops iterable into chunks. Blocks until all results are ready. Returns results in order.\n",
    "*   `pool.imap(func, iterable, chunksize=1)`: Like `map`, but returns an iterator yielding results as soon as they are ready. More memory efficient for large result sets.\n",
    "*   `pool.imap_unordered(func, iterable, chunksize=1)`: Like `imap`, but results are yielded as soon as they complete, regardless of input order.\n",
    "*   `pool.apply(func, args=(), kwds={})`: Executes `func(*args, **kwds)` in ONE worker process. Blocks until complete. Useful for single tasks.\n",
    "*   `pool.apply_async(func, args=(), kwds={}, callback=None, error_callback=None)`: Asynchronous version of `apply`. Returns an `AsyncResult` object immediately. `callback(result)` is called on success, `error_callback(exception)` on error.\n",
    "*   `pool.map_async(...)`, `pool.starmap_async(...)`: Asynchronous versions of map/starmap.\n",
    "*   `pool.close()`: Prevents new tasks from being submitted.\n",
    "*   `pool.join()`: Waits for worker processes to exit (must call `close()` or `terminate()` first).\n",
    "\n",
    "**Important:** Objects passed to/returned from pool workers must be pickleable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Multiprocessing Pool Demo ---\n",
      "Creating Pool with 16 workers.\n",
      "Using pool.map()...\n",
      "  Results (map): [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361]\n",
      "\n",
      "Using pool.imap_unordered()...\n",
      "  Results (imap_unordered, sorted): [0, 1, 4, 9, 16, 25, 36, 49, 64, 81, 100, 121, 144, 169, 196, 225, 256, 289, 324, 361]\n",
      "\n",
      "Using pool.apply_async()...\n",
      "  Results (apply_async): [10000, 40000, 90000]\n",
      "\n",
      "Pool execution finished in 0.17 seconds\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import time\n",
    "import os\n",
    "\n",
    "def calculate_square(n):\n",
    "    # Simulate CPU work\n",
    "    # time.sleep(0.01)\n",
    "    pid = os.getpid()\n",
    "    # print(f\"PID {pid} calculating square of {n}\") # Can be noisy\n",
    "    return n * n\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Multiprocessing Pool Demo ---\")\n",
    "    numbers_to_square = list(range(20)) # Example data\n",
    "    \n",
    "    # Determine pool size (often based on CPU count)\n",
    "    pool_size = os.cpu_count()\n",
    "    print(f\"Creating Pool with {pool_size} workers.\")\n",
    "    \n",
    "    start_pool = time.perf_counter()\n",
    "    \n",
    "    # Use Pool as context manager (automatically calls close/join)\n",
    "    with Pool(processes=pool_size) as pool:\n",
    "        print(\"Using pool.map()...\")\n",
    "        # map() distributes work and collects results in order\n",
    "        results_map = pool.map(calculate_square, numbers_to_square, chunksize=5)\n",
    "        print(f\"  Results (map): {results_map}\")\n",
    "        \n",
    "        print(\"\\nUsing pool.imap_unordered()...\")\n",
    "        # imap_unordered yields results as they finish (order not guaranteed)\n",
    "        results_imap = []\n",
    "        for result in pool.imap_unordered(calculate_square, numbers_to_square, chunksize=5):\n",
    "            # print(f\"  Got result (imap): {result}\") # Can be noisy\n",
    "            results_imap.append(result)\n",
    "        print(f\"  Results (imap_unordered, sorted): {sorted(results_imap)}\") # Sort for comparison\n",
    "        \n",
    "        # Example with apply_async (for individual tasks)\n",
    "        print(\"\\nUsing pool.apply_async()...\")\n",
    "        async_results = []\n",
    "        for num in [100, 200, 300]:\n",
    "             res_obj = pool.apply_async(calculate_square, args=(num,))\n",
    "             async_results.append(res_obj)\n",
    "             \n",
    "        # Retrieve results from AsyncResult objects\n",
    "        final_async_results = [ar.get() for ar in async_results] \n",
    "        print(f\"  Results (apply_async): {final_async_results}\")\n",
    "        \n",
    "    # Pool is automatically closed and joined here\n",
    "    \n",
    "    end_pool = time.perf_counter()\n",
    "    print(f\"\\nPool execution finished in {end_pool - start_pool:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Modern Approach: `concurrent.futures.ProcessPoolExecutor`\n",
    "\n",
    "Provides a high-level interface similar to `ThreadPoolExecutor`, but uses processes instead of threads. Often preferred for its simpler API compared to manually managing `Pool` or `Process` objects.\n",
    "\n",
    "**Key Methods:**\n",
    "*   `executor.submit(fn, *args, **kwargs)`: Submits a callable to be executed. Returns a `Future` object.\n",
    "*   `executor.map(func, *iterables, timeout=None, chunksize=1)`: Similar to `Pool.map`, applies `func` to items from iterables. Returns an iterator yielding results.\n",
    "*   `executor.shutdown(wait=True, *, cancel_futures=False)`: Signals the executor to stop accepting new tasks and shuts down (waits for pending tasks if `wait=True`). Automatically called when using a context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ProcessPoolExecutor Demonstration ---\n",
      "Submitting tasks using executor.map...\n",
      "  Results (map): [0, 1, 8, 27, 64, 125, 216, 343, 512, 729, 1000, 1331, 1728, 2197, 2744]\n",
      "\n",
      "ProcessPoolExecutor finished in 0.15 seconds\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function needs to be defined at top level or importable for ProcessPoolExecutor\n",
    "def calculate_cube(n):\n",
    "    pid = os.getpid()\n",
    "    # print(f\"PID {pid} calculating cube of {n}\")\n",
    "    # time.sleep(0.01) # Simulate work\n",
    "    return n * n * n\n",
    "\n",
    "if __name__ == \"__main__\": # Essential for ProcessPoolExecutor too\n",
    "    print(\"\\n--- ProcessPoolExecutor Demonstration ---\")\n",
    "    numbers_to_cube = list(range(15))\n",
    "    \n",
    "    start_ppe = time.perf_counter()\n",
    "    \n",
    "    # Use context manager for automatic shutdown\n",
    "    # max_workers defaults to os.cpu_count()\n",
    "    with ProcessPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        print(\"Submitting tasks using executor.map...\")\n",
    "        # map returns results in order, waits for all\n",
    "        results = list(executor.map(calculate_cube, numbers_to_cube, chunksize=4))\n",
    "        print(f\"  Results (map): {results}\")\n",
    "        \n",
    "        # Example using submit\n",
    "        # print(\"\\nSubmitting tasks using executor.submit...\")\n",
    "        # futures = [executor.submit(calculate_cube, num) for num in [100, 200, 300]]\n",
    "        # submit_results = [f.result() for f in futures]\n",
    "        # print(f\"  Results (submit): {submit_results}\")\n",
    "        \n",
    "    # Executor automatically shut down here\n",
    "    \n",
    "    end_ppe = time.perf_counter()\n",
    "    print(f\"\\nProcessPoolExecutor finished in {end_ppe - start_ppe:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Process Start Methods\n",
    "\n",
    "`multiprocessing` can use different methods to start child processes, configurable via `multiprocessing.set_start_method()` (must be called only once, ideally within the `if __name__ == '__main__':` block before creating processes/pools).\n",
    "\n",
    "*   **`fork` (Default on Unix/Linux):** Creates a child process by duplicating the parent process's memory space. \n",
    "    *   **Pros:** Very fast startup.\n",
    "    *   **Cons:** Can be problematic with threaded applications (only the thread calling `fork` exists in the child), issues with shared resources like file descriptors or locks acquired before forking, potential for copy-on-write performance hits.\n",
    "*   **`spawn` (Default on Windows/macOS, available on Unix):** Starts a fresh Python interpreter process. The child process only inherits necessary resources to run the target function, plus specified arguments.\n",
    "    *   **Pros:** Cleaner separation, avoids issues related to forking threaded applications.\n",
    "    *   **Cons:** Slower startup as a new interpreter needs to start and the necessary code/data needs to be imported/pickled.\n",
    "*   **`forkserver` (Available on Unix):** Starts a server process when the first process is created. Subsequent processes are forked from this server process.\n",
    "    *   **Pros:** Avoids issues with forking from threaded processes, potentially faster than `spawn` for subsequent process creation.\n",
    "    *   **Cons:** Still slower initial startup than `fork`.\n",
    "\n",
    "**Recommendation:** While `fork` is the default on Linux, `spawn` or `forkserver` are often considered safer, especially in complex or threaded applications, despite the startup overhead. `spawn` is generally the most portable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Process Start Method Demo ---\n",
      "Available start methods: ['fork', 'spawn', 'forkserver']\n",
      "Default start method: fork\n",
      "\n",
      "Running info_func in main process:\n",
      "Main line\n",
      "module name: __main__\n",
      "parent process: 7099\n",
      "process id: 10985\n",
      "Child process\n",
      "module name: __main__\n",
      "parent process: 10985\n",
      "process id: 11167\n",
      "Start method demo finished.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "import sys\n",
    "\n",
    "def info_func(title):\n",
    "    print(title)\n",
    "    print('module name:', __name__)\n",
    "    print('parent process:', os.getppid())\n",
    "    print('process id:', os.getpid())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"--- Process Start Method Demo ---\")\n",
    "    # Get available methods\n",
    "    print(f\"Available start methods: {mp.get_all_start_methods()}\")\n",
    "    # Get current default method\n",
    "    print(f\"Default start method: {mp.get_start_method()}\")\n",
    "\n",
    "    # --- Set start method (Optional - do this BEFORE creating processes/pools) ---\n",
    "    # Generally only needed if you want to override the default for your OS.\n",
    "    # try:\n",
    "    #     if sys.platform != 'win32': # spawn is default on Windows\n",
    "    #          mp.set_start_method('spawn')\n",
    "    #          print(f\"Start method set to: {mp.get_start_method()}\")\n",
    "    #     else:\n",
    "    #          print(\"Cannot set start method easily after context might be initialized on Windows.\")\n",
    "    # except (RuntimeError, ValueError) as e:\n",
    "    #     # RuntimeError if called more than once or after context initialized\n",
    "    #     # ValueError if method not available\n",
    "    #     print(f\"Could not set start method: {e}\")\n",
    "    \n",
    "    print(\"\\nRunning info_func in main process:\")\n",
    "    info_func('Main line')\n",
    "    \n",
    "    p = mp.Process(target=info_func, args=('Child process',))\n",
    "    p.start()\n",
    "    p.join()\n",
    "    \n",
    "    print(\"Start method demo finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices & Enterprise Considerations\n",
    "\n",
    "1.  **Use for CPU-Bound Tasks:** Multiprocessing excels where the GIL limits threading.\n",
    "2.  **Guard Main Module:** Always protect the script entry point with `if __name__ == '__main__':`.\n",
    "3.  **Prefer High-Level APIs:** Use `concurrent.futures.ProcessPoolExecutor` or `multiprocessing.Pool` over manual `Process` management where possible, as they handle worker lifecycle and task distribution.\n",
    "4.  **Minimize IPC:** Inter-process communication (Queues, Pipes, Managers) involves overhead (serialization, OS calls). Design to minimize data transfer between processes. Pass simple data or references if possible.\n",
    "5.  **Pickleable Objects:** Ensure data passed between processes (via Queues, Pipes, Pool arguments/results) is pickleable. Complex objects, closures, or generators might not be.\n",
    "6.  **Resource Management:** Ensure processes release resources (files, connections) properly, especially if using `daemon=True` or `pool.terminate()`.\n",
    "7.  **Error Handling:** Exceptions in child processes don't automatically propagate to the parent unless using mechanisms like `Pool` or `Executor` results (`.get()`, iterating `map` results).\n",
    "8.  **Synchronization:** Use `multiprocessing` locks/events when using shared memory (`Value`, `Array`) or coordinating processes.\n",
    "9.  **Consider Start Methods:** Be aware of the implications of `fork` vs. `spawn` vs. `forkserver`, especially regarding resource inheritance and compatibility with other libraries (like some GUI toolkits or network libraries).\n",
    "10. **Logging:** Configuring logging across multiple processes requires care (e.g., using separate files per process, a queue handler to send logs to a central logging process, or process-aware formatters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Pitfalls and Common Interview Questions\n",
    "\n",
    "**Common Pitfalls:**\n",
    "\n",
    "*   **Forgetting `if __name__ == '__main__':`:** Leading to recursive process creation or errors, especially on Windows/macOS with `spawn`.\n",
    "*   **Serialization (Pickling) Errors:** Trying to pass unpickleable objects (lambdas, complex local objects, generators) between processes.\n",
    "*   **IPC Overhead:** Excessive communication between processes becoming a bottleneck.\n",
    "*   **Race Conditions with Shared Memory:** Using `Value` or `Array` without proper locking.\n",
    "*   **Deadlocks:** Processes waiting for resources/locks held by each other.\n",
    "*   **Resource Leaks:** Child processes not cleaning up resources properly.\n",
    "*   **Zombie Processes:** Parent process not `join()`ing child processes, leaving them in the system process table (less common with high-level APIs).\n",
    "*   **Complexity:** Managing IPC and synchronization can be significantly more complex than in threading.\n",
    "\n",
    "**Common Interview Questions:**\n",
    "\n",
    "1.  Why would you use multiprocessing instead of threading in Python?\n",
    "2.  How does multiprocessing bypass the GIL?\n",
    "3.  What does the `if __name__ == '__main__':` guard do, and why is it essential for multiprocessing?\n",
    "4.  How can processes share data? Describe at least two IPC mechanisms (`Queue`, `Pipe`, `Value`, `Array`, `Manager`).\n",
    "5.  What are the trade-offs between different IPC methods (e.g., Queue vs. Shared Memory)?\n",
    "6.  What is `multiprocessing.Pool` used for?\n",
    "7.  What is the difference between `pool.map()` and `pool.apply_async()`?\n",
    "8.  What is `concurrent.futures.ProcessPoolExecutor`?\n",
    "9.  What are process start methods (`fork`, `spawn`), and what are their implications?\n",
    "10. Can you pass complex objects like database connections between processes directly? Why or why not? (Usually not - pickling issues, resource handles are process-specific)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Challenge: Parallel Prime Number Calculation\n",
    "\n",
    "**Goal:** Find all prime numbers within a given range using multiple processes to speed up the calculation.\n",
    "\n",
    "**Tasks:**\n",
    "\n",
    "1.  **Primality Test Function:** Create a reasonably efficient function `is_prime(n: int) -> bool` that checks if a number `n` is prime. (For numbers > 2, check divisibility only by odd numbers up to `sqrt(n)`).\n",
    "2.  **Define Range and Chunk Size:** Choose a large range (e.g., 1 to 2,000,000) and decide how to split it among worker processes (e.g., determine chunk size based on number of CPU cores).\n",
    "3.  **Worker Function:** Create a function `find_primes_in_range(start: int, end: int) -> List[int]` that takes a start and end value and returns a list of all prime numbers found within that sub-range `[start, end)` by calling `is_prime()`.\n",
    "4.  **Parallel Execution:**\n",
    "    *   Use `concurrent.futures.ProcessPoolExecutor` (recommended) or `multiprocessing.Pool`.\n",
    "    *   Divide the total range into chunks.\n",
    "    *   Submit the `find_primes_in_range` function to the executor/pool for each chunk.\n",
    "5.  **Collect Results:** Gather the lists of primes returned from each worker process.\n",
    "6.  **Combine and Sort:** Combine all the lists into a single list of primes found in the total range. Sort the final list.\n",
    "7.  **Output:** Print the total number of primes found and maybe the first/last few primes.\n",
    "8.  **Timing:** Measure the time taken for the parallel execution.\n",
    "\n",
    "**(Bonus):** Implement the same calculation sequentially and compare the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] (MainProcess) Calculating primes up to 1000000\n",
      "[INFO] (MainProcess) Using 16 processes with chunk size ~62500\n",
      "[INFO] (MainProcess) Submitting task for range [1, 62501)\n",
      "[INFO] (MainProcess) Submitting task for range [62501, 125001)\n",
      "[INFO] (MainProcess) Submitting task for range [125001, 187501)\n",
      "[INFO] (MainProcess) Submitting task for range [187501, 250001)\n",
      "[INFO] (MainProcess) Submitting task for range [250001, 312501)\n",
      "[INFO] (MainProcess) Submitting task for range [312501, 375001)\n",
      "[INFO] (MainProcess) Submitting task for range [375001, 437501)\n",
      "[INFO] (MainProcess) Submitting task for range [437501, 500001)\n",
      "[INFO] (MainProcess) Submitting task for range [500001, 562501)\n",
      "[INFO] (MainProcess) Submitting task for range [562501, 625001)\n",
      "[INFO] (MainProcess) Submitting task for range [625001, 687501)\n",
      "[INFO] (MainProcess) Submitting task for range [687501, 750001)\n",
      "[INFO] (MainProcess) Submitting task for range [750001, 812501)\n",
      "[INFO] (MainProcess) Submitting task for range [812501, 875001)\n",
      "[INFO] (MainProcess) Submitting task for range [875001, 937501)\n",
      "[INFO] (MainProcess) Submitting task for range [937501, 1000001)\n",
      "[INFO] (MainProcess) Waiting for results...\n",
      "[INFO] (MainProcess) Received 6275 primes from range [1, 62501)\n",
      "[INFO] (MainProcess) Received 5459 primes from range [62501, 125001)\n",
      "[INFO] (MainProcess) Received 5230 primes from range [125001, 187501)\n",
      "[INFO] (MainProcess) Received 5080 primes from range [187501, 250001)\n",
      "[INFO] (MainProcess) Received 4948 primes from range [250001, 312501)\n",
      "[INFO] (MainProcess) Received 4852 primes from range [375001, 437501)\n",
      "[INFO] (MainProcess) Received 4912 primes from range [312501, 375001)\n",
      "[INFO] (MainProcess) Received 4719 primes from range [500001, 562501)\n",
      "[INFO] (MainProcess) Received 4782 primes from range [437501, 500001)\n",
      "[INFO] (MainProcess) Received 4729 primes from range [562501, 625001)\n",
      "[INFO] (MainProcess) Received 4612 primes from range [687501, 750001)\n",
      "[INFO] (MainProcess) Received 4640 primes from range [625001, 687501)\n",
      "[INFO] (MainProcess) Received 4635 primes from range [750001, 812501)\n",
      "[INFO] (MainProcess) Received 4558 primes from range [875001, 937501)\n",
      "[INFO] (MainProcess) Received 4492 primes from range [937501, 1000001)\n",
      "[INFO] (MainProcess) Received 4575 primes from range [812501, 875001)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Parallel Calculation Results ---\n",
      "Total primes found up to 1000000: 78498\n",
      "First 10 primes: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29]\n",
      "Last 10 primes: [999863, 999883, 999907, 999917, 999931, 999953, 999959, 999961, 999979, 999983]\n",
      "Time taken (multiprocessing): 0.73 seconds\n"
     ]
    }
   ],
   "source": [
    "# --- Solution Space for Challenge ---\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from typing import List, Tuple\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='[%(levelname)s] (%(processName)s) %(message)s',\n",
    "                    force=True)\n",
    "\n",
    "# 1. Primality Test Function\n",
    "def is_prime(n: int) -> bool:\n",
    "    \"\"\"Checks if a number is prime.\"\"\"\n",
    "    if n < 2:\n",
    "        return False\n",
    "    if n == 2:\n",
    "        return True\n",
    "    if n % 2 == 0:\n",
    "        return False\n",
    "    # Check odd divisors up to sqrt(n)\n",
    "    sqrt_n = int(math.sqrt(n)) + 1\n",
    "    for i in range(3, sqrt_n, 2):\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# 3. Worker Function\n",
    "def find_primes_in_range(start: int, end: int) -> Tuple[int, int, List[int]]:\n",
    "    \"\"\"Finds primes in the range [start, end) and returns (start, end, primes_list).\"\"\"\n",
    "    # logging.info(f\"Checking range [{start}, {end})...\") # Can be very noisy\n",
    "    primes = [n for n in range(start, end) if is_prime(n)]\n",
    "    # logging.info(f\"Finished range [{start}, {end}). Found {len(primes)} primes.\")\n",
    "    return start, end, primes\n",
    "\n",
    "# --- Main Execution Guard ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 2. Define Range and Chunk Size\n",
    "    MAX_NUMBER = 1_000_000 # Adjust as needed (2M might take a while)\n",
    "    NUM_PROCESSES = max(1, os.cpu_count() or 1)\n",
    "    # Calculate chunk size - ensure it divides the range reasonably\n",
    "    chunk_size = (MAX_NUMBER + NUM_PROCESSES - 1) // NUM_PROCESSES # Ceiling division\n",
    "    \n",
    "    logging.info(f\"Calculating primes up to {MAX_NUMBER}\")\n",
    "    logging.info(f\"Using {NUM_PROCESSES} processes with chunk size ~{chunk_size}\")\n",
    "\n",
    "    # 4. Parallel Execution & 5. Collect Results\n",
    "    start_time_mp = time.perf_counter()\n",
    "    all_found_primes = []\n",
    "    tasks = []\n",
    "\n",
    "    with ProcessPoolExecutor(max_workers=NUM_PROCESSES) as executor:\n",
    "        # Create tasks for each chunk\n",
    "        for i in range(NUM_PROCESSES):\n",
    "            range_start = i * chunk_size + 1 # Start from 1\n",
    "            # Ensure range_start doesn't exceed MAX_NUMBER\n",
    "            range_start = max(1, range_start)\n",
    "            range_end = min(MAX_NUMBER + 1, range_start + chunk_size)\n",
    "            \n",
    "            # Avoid submitting empty ranges if chunking is uneven\n",
    "            if range_start >= range_end:\n",
    "                 continue \n",
    "                 \n",
    "            logging.info(f\"Submitting task for range [{range_start}, {range_end})\")\n",
    "            future = executor.submit(find_primes_in_range, range_start, range_end)\n",
    "            tasks.append(future)\n",
    "\n",
    "        # Collect results as they complete\n",
    "        logging.info(\"Waiting for results...\")\n",
    "        for future in as_completed(tasks):\n",
    "            try:\n",
    "                s, e, primes_in_chunk = future.result()\n",
    "                logging.info(f\"Received {len(primes_in_chunk)} primes from range [{s}, {e})\")\n",
    "                all_found_primes.extend(primes_in_chunk)\n",
    "            except Exception as exc:\n",
    "                 # Log any exception raised by the worker function\n",
    "                logging.error(f\"A task generated an exception: {exc}\")\n",
    "    \n",
    "    # 6. Combine and Sort (already combined, just sort)\n",
    "    all_found_primes.sort()\n",
    "    \n",
    "    end_time_mp = time.perf_counter()\n",
    "\n",
    "    # 7. Output\n",
    "    print(\"\\n--- Parallel Calculation Results ---\")\n",
    "    print(f\"Total primes found up to {MAX_NUMBER}: {len(all_found_primes)}\")\n",
    "    print(f\"First 10 primes: {all_found_primes[:10]}\")\n",
    "    print(f\"Last 10 primes: {all_found_primes[-10:]}\")\n",
    "    print(f\"Time taken (multiprocessing): {end_time_mp - start_time_mp:.2f} seconds\")\n",
    "\n",
    "    # Bonus: Sequential Comparison\n",
    "    # print(\"\\n--- Sequential Calculation (for comparison) ---\")\n",
    "    # start_time_seq = time.perf_counter()\n",
    "    # _, _, sequential_primes = find_primes_in_range(1, MAX_NUMBER + 1)\n",
    "    # end_time_seq = time.perf_counter()\n",
    "    # print(f\"Total primes found (sequential): {len(sequential_primes)}\")\n",
    "    # print(f\"Time taken (sequential): {end_time_seq - start_time_seq:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "Python's `multiprocessing` module unlocks true parallelism, allowing you to bypass the GIL and fully utilize multiple CPU cores for computationally intensive tasks. While the basic `Process` interface mirrors `threading`, the key difference lies in separate memory spaces, necessitating the use of IPC mechanisms like Queues, Pipes, Shared Memory, or Managers for communication.\n",
    "\n",
    "The high-level `multiprocessing.Pool` and `concurrent.futures.ProcessPoolExecutor` significantly simplify distributing tasks across worker processes. Choosing the right IPC method, understanding serialization limitations, and guarding the main module are crucial for successful multiprocessing.\n",
    "\n",
    "By applying multiprocessing effectively, you can achieve substantial performance gains for CPU-bound problems in Python."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
